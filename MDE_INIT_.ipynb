{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b340688e-acd8-484f-a980-669fcb7f567a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ingest from Source\n",
    "### I usually have all the functions to interact with the data source in the _INIT_ including Authentication, token refresh, and secret retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9ab16e1-25cb-4bc5-b69c-555f576b9703",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#create load function\n",
    "def ingestRefData(fileName):\n",
    "\n",
    "    # File location and type\n",
    "    file_location = f\"/FileStore/tables/MDE/{fileName}\"\n",
    "    file_type = \"csv\"\n",
    "\n",
    "    # CSV options\n",
    "    infer_schema = \"false\"\n",
    "    first_row_is_header = \"true\"\n",
    "    delimiter = \",\"\n",
    "\n",
    "    # The applied options are for CSV files. For other file types, these will be ignored.\n",
    "    df = spark.read.format(file_type) \\\n",
    "        .option(\"inferSchema\", infer_schema) \\\n",
    "        .option(\"header\", first_row_is_header) \\\n",
    "        .option(\"sep\", delimiter) \\\n",
    "        .load(file_location)\n",
    "\n",
    "    tableName = fileName[:-4]\n",
    "    #I print to grab the names of files for review- when pushing to PROD, the manual interventions go away\n",
    "    print(f'{tableName} created into spark dataframe')\n",
    "\n",
    "    #turn into delta view for temp storage- no need for these to be in perm/table storage\n",
    "    df.createOrReplaceTempView(tableName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0671c00e-4c14-440f-9c17-818b494d7197",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def IngestClinicalFiles(urlList):\n",
    "    # Initialize the schema and DataFrame\n",
    "    schema = None\n",
    "    df = None\n",
    "\n",
    "    # Iterate through the list; for optimization look at multiprocessing\n",
    "    for url in urlList:\n",
    "        res = requests.get(url)\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".xml\") as temp_file:\n",
    "            temp_file.write(res.content)\n",
    "            temp_file_path = temp_file.name\n",
    "\n",
    "        # Move the temporary file to DBFS\n",
    "        dbfs_path = f\"/dbfs/tmp/{temp_file.name.split('/')[-1]}\"\n",
    "        shutil.move(temp_file_path, dbfs_path)\n",
    "\n",
    "        # Load the XML file from DBFS\n",
    "        df2 = (spark.read\n",
    "            .format('xml')\n",
    "            .option('rowTag', 'ClinicalDocument')\n",
    "            #.option('excludeAttribute', 'true')  # Exclude attributes to avoid column name conflicts\n",
    "            .schema(inferred_schema)\n",
    "            .load(f\"dbfs:/tmp/{temp_file.name.split('/')[-1]}\"))\n",
    "        \n",
    "        if df is None:\n",
    "            df = df2\n",
    "        else:\n",
    "            df = df.unionByName(df2, allowMissingColumns=True)\n",
    "        print(f'Added {x}')\n",
    "\n",
    "    display(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd4983d0-ee2f-4c05-bf95-f69628467cf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# These functions flatten, explode, and normalize the pyspark dataframe schema\n",
    "### Doesn't work great with HL7 xml parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d1fe8bc-55f2-4be9-b770-3e7155f7e5c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "def flatten_schema(schema, prefix=None):\n",
    "    fields = []\n",
    "    for field in schema.fields:\n",
    "        name = f\"{prefix}.{field.name}\" if prefix else field.name\n",
    "        if isinstance(field.dataType, StructType):\n",
    "            fields += flatten_schema(field.dataType, prefix=name)\n",
    "        else:\n",
    "            fields.append(name)\n",
    "    return fields\n",
    "\n",
    "def explode_df(df):\n",
    "    for (name, dtype) in df.dtypes:\n",
    "        if \"array\" in dtype:\n",
    "            df = df.withColumn(name, explode(name))\n",
    "    return df\n",
    "\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "def is_flat(df):\n",
    "    for (_, dtype) in df.dtypes:\n",
    "        if \"array\" in dtype or \"struct\" in dtype:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def flatten_nest_df(df):\n",
    "    keep_going = True\n",
    "    while(keep_going):\n",
    "        fields = flatten_schema(df.schema)\n",
    "        new_fields = [item.replace(\".\", \"_\").lower() for item in fields]\n",
    "        df = df.select(fields).toDF(*new_fields)\n",
    "        df = explode_df(df)\n",
    "        if is_flat(df):\n",
    "            keep_going = False\n",
    "    return df\n",
    "#dfx = flatten_nest_df(df) Input pyspark dataframe of xml and run"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MDE_INIT_",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}